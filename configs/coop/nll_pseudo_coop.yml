model:
  latent_dim: 2048
  proj_dropout: 0.1
  linear_projection: true
  model_path: "johngiorgi/declutr-base"
  model_arch: "roberta"
  encoder_type: "SumTextEncoder"
  momentum: 0.0
  device: "cuda"
  labels: ['chapter', 'scene', 'description', 'character names', 'paragraph', 'dialogue', 'punctuation', 'lines', 'sentences', 'pov', 'character asking question', 'story', 'conversation', 'cut this', 'backstory', 'single character', 'emotional response to story event', 'openings and endings', 'use of action', 'character age', 'humor']
  loss: 'nll'

train_job:
  n_ctx: 512
  epochs: 30
  batch_size: 256
  microbatch_size: 256
  lr_ramp_steps: 50
  lr_decay_steps: 3366
  learning_rate_init: 5.0e-5
  learning_rate_target: 0.000006
  log_interval: 25
  checkpoint_interval: 100000
  validate_interval: 30
  use_half: false
  do_log: true
  validation_size: 32
  eval_selection: "final_n"
  use_bucket: false
  dupe_protection: true
  hard_dupe_protection: false
  data_pipeline: "MetalabelDataPipeline"
  trainer: "CARPCoOpTrainer"
  gradient_checkpointing: true
  grad_clip: -1.0
  grad_accum: 2
  temp: 1 #Softmax temperature
  save_folder: "nll_smooth"
  save_best_val: false
  save_at_end: true